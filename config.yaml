rd_alpha: 
  desc: The first minimal-viable-product of wisdomify. Trained on wisdom2def only. S_wisdom = S_wisdom_literal
  a:
    bert: beomi/kcbert-base
    desc: The first version of RDAlpha - trained on `wisdom2def:a`
    seed: 410
    train_type: wisdom2def
    train_ver: a
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 2
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
  b: 
    bert: beomi/kcbert-base
    desc: The second version RDAlpha - trained on `wisdom2def:b`
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # this is an optimised value: refer to: magic-water-909 
    lr: 0.000003631
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
  b_high_lr:
    bert: beomi/kcbert-base
    desc: rd_alpha with high learning rate
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # this is an optimised value: refer to: magic-water-909
    lr: 0.001
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
  c: 
    bert: kykim/bert-kor-base
    desc: the second version of RDBeta - trained on `wisdom2def:b`
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
rd_beta: 
  desc: The second version of RD. S_wisdom = S_wisdom_literal + S_wisdom_figurative.
  a: 
    bert: beomi/kcbert-base
    desc: the first version of RDBeta - trained on `wisdom2def:a`
    seed: 410
    train_type: wisdom2def
    train_ver: a
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
  b: 
    bert: beomi/kcbert-base
    desc: the second version of RDBeta - trained on `wisdom2def:b`
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # refer to: apricot-galaxy-911
    lr: 0.000001445
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
  b_high_lr:
    bert: beomi/kcbert-base
    desc: rdbeta with high learning rate
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # this is an optimised value: refer to: magic-water-909
    lr: 0.001
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 0
    stochastic_weight_avg: true
rd_gamma:
  desc: The third version of RD. S_wisdom = S_wisdom_literal + S_wisdom_figurative (much simplified)
  a:
    bert: beomi/kcbert-base
    desc: the first version of RDGamma - trained on `wisdom2def:a`
    seed: 410
    train_type: wisdom2def
    train_ver: a
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    pooler_size: 30
    loss_func: cross_entropy
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
  a_sync:
    bert: beomi/kcbert-base
    desc: ...
    seed: 410
    train_type: wisdom2def
    train_ver: a
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    pooler_size: 30
    loss_func: cross_entropy_with_sync
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
  b:
    bert: beomi/kcbert-base
    desc: ...
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    pooler_size: 30
    loss_func: cross_entropy
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
  b_sync:
    bert: beomi/kcbert-base
    desc: ...
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    lr: 0.00001
    pooler_size: 30
    loss_func: cross_entropy_with_sync
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
  b_sync_high_lr:
    bert: beomi/kcbert-base
    desc: ...
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # ten times bigger learning rate
    lr: 0.0001
    pooler_size: 30
    loss_func: cross_entropy_with_sync
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
  b_sync_optim_lr:
    bert: beomi/kcbert-base
    desc: ...
    seed: 410
    train_type: wisdom2def
    train_ver: b
    wisdoms_ver: a
    val_test_ver: a
    k: 11
    # ten times bigger learning rate
    # refer to: misty-bush-925
    lr: 0.000002089
    pooler_size: 30
    loss_func: cross_entropy_with_sync
    max_epochs: 200
    batch_size: 64
    num_workers: 4
    shuffle: true
    log_every_n_steps: 1
    enable_checkpointing: false
    num_sanity_val_steps: 1
    stochastic_weight_avg: true
